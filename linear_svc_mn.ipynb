{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f2d807-b693-4430-b003-7022dba16b77",
   "metadata": {},
   "source": [
    "# Assignment 2: Scalability of Support Vector Machines\n",
    "## Miles Nordwall, Nathan Nail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6ee33f-c0cf-4afd-b57a-151c475b3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9d1fc-1190-452f-bc66-5380f87bea12",
   "metadata": {},
   "source": [
    "### Task #1 Implement LinearSVC\n",
    "doesn't work yet, need to finish task 2 so i can test the implementation\n",
    "- [ ] missing weight update rule\n",
    "- [ ] there are a bunch of todos just go through those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83771d9-ab82-402e-af36-d8f50975090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVC(object):\n",
    "    \"\"\"\n",
    "    Linear Support Vector Classifier\n",
    "    Params: \n",
    "    eta (float) : Learning rate (between 0.0 and 1.0)\n",
    "    n_iter (int) : Number of passes over the training dataset.\n",
    "    random_state (int) : Random number generator seed for random weight initialization.\n",
    "\n",
    "    Attrs:\n",
    "    w_ (1d-array) : Weights after fitting with bias absorbed at X[0].\n",
    "    losses_ (list) : Hinge Loss with L2-regularization loss function values in each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.w_ = []\n",
    "        self.rand_gen = []\n",
    "        self.w_initialized = False\n",
    "\n",
    "    def fit(self, X, y, C=1.0):\n",
    "        \"\"\"\n",
    "        Fitting Training Data\n",
    "        Params:\n",
    "        X {array-like}, shape = [n_examples, n_features]\n",
    "        Training vectors, where n_examples is the number of examples and n_features is the number of features.\n",
    "        y (array-like) ,shape = [n_examples]\n",
    "        Target values.\n",
    "        C {float} = regularization hyperparameter \n",
    "\n",
    "        Returns:\n",
    "        self : Instance of LinearSVC\n",
    "        \"\"\"\n",
    "        self.w_ = []\n",
    "        self.losses_ = []\n",
    "\n",
    "        X = self._initialize_weights(X,X.shape[1])\n",
    "\n",
    "        c_n = np.ceil(C /n)\n",
    "        for _ in range(self.n_iter):\n",
    "            for xi, target in zip(X, y):\n",
    "                \n",
    "                \n",
    "                loss = Li * c_n + (0.5 * (self.w_.shape[0])^2)\n",
    "                self.losses_.append(self._update_weights(xi, target))\n",
    "        \n",
    "        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    \n",
    "    def _initialize_weights(self, X, m):\n",
    "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
    "        n = X.shape[0] \n",
    "        x_0 = np.ones((n,-1)) #TODO2 does making x_0 all -1 fix the TODO1?\n",
    "        X = np.hstack((x_0,X))\n",
    "        \n",
    "        self.rand_gen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = self.rand_gen.normal(loc=0.0, scale=0.01,\n",
    "                                  size=1 + m) #m + 1 to account for the\n",
    "                                              #absorbed bias\n",
    "        self.w_initialized = True\n",
    "        return X\n",
    "        \n",
    "\n",
    "    def _update_weights(self, xi, y):\n",
    "        \"\"\"Apply linear svc learning rule to update the weights\"\"\"\n",
    "        output = self.activation(self.net_input(xi),target=y)\n",
    "\n",
    "        #TODO do either QP or gradient descent to get error (prof mentioned QP!)\n",
    "        #error = (y - output)\n",
    "\n",
    "        self.w_  += self.eta * 2.0 * xi #* (error) TODO what the new update rule? \n",
    "        loss = output * c_n + (0.5 * (self.w_.shape[0])^2)\n",
    "        return loss\n",
    "\n",
    "    def net_input(self, X): #TODO1 the net_input for svc is w^Tx-b right? unabsorb b?\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_)\n",
    "\n",
    "    def activation(self, y_hat,target):\n",
    "        \"\"\"Compute Hinge Loss\"\"\"\n",
    "        return np.max(0, 1 - (target*y_hat))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        n = X.shape[0]\n",
    "        m = X.shape[1] #number of features in dataset\n",
    "        x_0 = np.ones((n,1))\n",
    "        X = np.hstack((x_0,X))\n",
    "\n",
    "        #TODO should this really be 0.5?? or does the qp part go here?\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, -1)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2feef-243e-4243-9373-6d392e9f850b",
   "metadata": {},
   "source": [
    "### Task #2 make_classification function\n",
    "$$\\bar{a}^T \\bar{x} = b$$\n",
    "$$\\{ \\bar{x} \\in \\mathbb{R}^d | \\bar{a}^T \\bar{x} = b\\}$$\n",
    "$$\\text{where } b=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f002d2-fdc7-4b79-aff0-c7b6cc905e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      "[[ -80.21728386  -44.88778077 -110.5935076 ]\n",
      " [-165.45154524 -236.34686048  113.53453479]\n",
      " [-101.70141365   63.73618141  -85.99066067]]\n",
      "[-152.64280512 -188.95833506   35.65194485]\n",
      "[-1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "def make_classification(d,n, u=100,seed=1):\n",
    "    \"\"\"\n",
    "    Generates a set of linearly seperable data\n",
    "    based on a random seperation hyperplane\n",
    "    d (int) : dimension of set of points\n",
    "    \"\"\"\n",
    "    rand_gen = np.random.RandomState(seed)\n",
    "    #generate random vector a s.t ||a||=d\n",
    "    a = rand_gen.random(size=d) #TODO values are between [0-1] btw, is that okay?\n",
    "    \n",
    "    #randomly select n samples in range of [-u,u] in each dimension using a gaussian\n",
    "    x = []\n",
    "    for i in range(n):\n",
    "        x_gauss = rand_gen.normal(scale=u,size=d)\n",
    "        x.append(x_gauss)\n",
    "    X = np.array(x)\n",
    "    #give each xi a label yi \n",
    "    y = []\n",
    "    for j in range(n):\n",
    "        if a.T.dot(X[j]) < 0: #is this correct?\n",
    "            yi = -1\n",
    "        else:\n",
    "            yi = 1\n",
    "        y.append(yi)\n",
    "    Y = np.array(y)\n",
    "\n",
    "    #TODO make the first 70% training, and the last 30% test\n",
    "\n",
    "    print(a)\n",
    "    print(X)\n",
    "    print(a.T.dot(X))\n",
    "    print(Y)\n",
    "    \n",
    "\n",
    "make_classification(d=3,n=3,seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0caf3-0092-481b-9d56-c77152e16314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
